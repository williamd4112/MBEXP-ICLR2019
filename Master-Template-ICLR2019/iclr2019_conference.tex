\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\include{custom_package}


\title{Model-based Lookahead Exploration for Deep Reinforcement Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
% The role of exploration in RL
% Simple approach of exploration: eps-greedy and entropy-regularize; too simple to achieve satisfactory behaviors
% Complex approach of exploration: information gain, counting-table, neural density model, and prediction error; effective but they are based on model-free framework, it is inefficient.
% The inefficiency prevents these exploration strategy to be employed in the scenarios where only limited amount of interactions are allowed. Exploration remains a challenge
Exploration plays a key role in deep reinforcement learning (DRL) as it enables DRL agents to discover superior policies in Markov Decision Process (MDP). Typically, a simple heuristic such as $\epsilon$-greedy \cite{mnih2015human}, the entropy regularization principle \cite{mnih2016a3c}, and parameter space noise~\cite{plappert2018paramnoise,fortunato2018noisy} are served as exploration strategies of most of DRL algorithms. Despite of the simplicity of these approaches, these undirected exploration usually require a larger amount of data than directed ones. A more sophisticated line of works in directed exploration drive RL agents to visit novel states by bonus rewards. Bonus rewards can be estimated by information gain of state transitions~\cite{houthooft2016vime}, hashing mechanism~\cite{tang2017exploration}, density model ~\cite{bellemare2016unifying, ostrovski2017count}, and prediction errors~\cite{stadie2015incentivizing,pathak2017curiosity}. Although such sophisticated approaches have shown their effectiveness in several domains (e.g. Atari 2600~\cite{bellemare13arcade}, MuJoCo~\cite{todorov2012mujoco}), the model-free RL (MFRL) nature of these methods prevents them from being practical in the real world due to the data-inefficiency of MFRL.

% An more efficient alternative to solve reinforcement learning problem is model-based RL
% MBRL learns a forward dynamic model, train agents or make plans with this model
% PILCO (MBPS) train agents with virtual rollouts
% MBRL have demonstrated superior efficiency, but their resulting performance are inferior to MFRL
% Dyna trains agents with virtual and real data jointly
% NN-dynamics initializes MFRL learner with a random-shooting MPC
% Though they have shown the significant advantages on data-efficiency, none of works investigate the combination of exploration and model-based approaches.
A more efficient alternative to solve MDP is model-based reinforcement learning (MBRL). MBRL learns a forward dynamic model of the environment for policy learning~\cite{deisenroth2011pilco} or planning~\cite{kamthe2018gpmpc}. For instance, in~\cite{deisenroth2011pilco}, a forward dynamic model is trained to search the policy using the model without interactions with environments. In spite of its remarkable data-efficiency, the resulting performance loses to model-free RL's. One promising way to mitigate the performance gap between MBRL and MFRL is hybrid of MBRL and MFRL. For example, the authors of \cite{sutton2012dyna, kurutach2018modelensemble} train RL agents with fictitious data generated by forward dynamic models and real data from environments. On the other hand, in~\cite{nagabandi2017neural}, the model-free RL agent is initialized with model predictive controller (MPC) and results in a substantial improvement of data-efficiency.
In addition, in~\cite{silver2016mastering,tamar2016value}, MFRL policies are incorporated with planning algorithms such as monte-carlo tree Search (MCTS) and value iteration to enhance the performance.
Though a plethora of works have shown the successes of MBRL, the advantages of model-based approaches in exploration are unexplored.

% Motivated by the considerable data-efficiency demonstrated by hybrid of MBRL and MFRL, we propose to adopt model-based planning approaches to improve exploration.
% Shortcoming of old methods
% How can model-based planning improve it
Motivated by the impressive data-efficiency demonstrated by MBRL, we propose to adopt model-based planning approaches to improve the efficiency of exploration. The major shortcoming of the contemporary exploration strategies is the extraordinary sample complexity brought by the model-free nature. MFRL agents are blind to the actions which result in the lower long-term rewards, which leads to the poor data-efficiency. In contrast, model-based planning is able to foresee the consequences of actions by forward simulation. Moreover, it is known that learning forward dynamic models of environments requires less data than learning policies~\cite{nagabandi2017neural}.  Hence, incorporating model-based planning with exploration is a promising way to mitigate the inefficiency of exploration in RL.

% Our method
% How it works
In this paper, we present a model-based lookahead  exploration (MBLE), an exploration strategy which leverages the foresight capability of MBRL to assist MFRL exploration. Our methodology can be implemented on the top of on- and off- policy MFRL algorithms. MBLE performs the exploratory behaviors at each timestep by model-based planning. During planning, multiple virtual trajectories are generated by the stochastic MFRL policy and the forward dynamic model. Each virtual trajectory starts from the current state of the environment. This state will be propagated through the policy and the forward dynamic model, resulting the following states and actions in the virtual trajectory. At the end, the actual action taken is the first action in the best trajectory. The best virtual trajectory is randomly chosen from the top-$K$ ones according to the accumulated rewards of the virtual trajectory. The reward at each state is given by the specified reward function. On the other hand, taking only the first action has been used to ease the compounding errors induced by the long-term planning~\cite{nagabandi2017neural}.
% Compare to previous works
% Exploration: prev are model-free, no planning
% MBRL: we study exploratory behaviors, not training agents with virtual data and real data
% Our method is novel
Our methodology differentiates to previous methodologies in several aspects. Firstly, comparing to previous exploration strategies which rely on MFRL for action selection, our method saves the time on trying worthless actions blindly by model-based planning. Next, though MFRL has been adopted in planning in~\cite{silver2016mastering,tamar2016value}, their methods are limited in simple dynamics (i.e. board game) or well-structured state space. In contrast, our approach is able to work in continuous, high-dimensional, and stochastic state/action space without limitations. Therefore, the proposed methodology is novel in either exploration and MBRL literature.
% Eval proc
% Simple dynamic
% Complex dynamic
% Ablative study
To validate the effectiveness of MBLE, we compare the performance of MBLE and several baselines on a range of continuous control tasks in MuJoCo. MBLE is implemented on DDPG and TRPO. The baselines include the MBRL approach proposed in~\cite{nagabandi2017neural} and the vanilla DDPG and TRPO. The experimental results show that MBLE does lead to better data-efficiency than all of the baselines. In most of tasks, MBLE achieves the same or superior level of performance than the baselines reach at the convergence. 

% The contributions of our method
The main contributions of this paper are summarized as follows:
\begin{itemize}
    \item An data-efficient exploration strategy for on- and off- policy MFRL algorithms.
    \item A novel exploration strategy based on model-based planning in the environment with complex dynamics.
    \item Combining MFRL policy with model-based planning in continuous control tassk.
\end{itemize}
The remainder of this paper is organized as follows. Section 2 presents background material that is required to understand the ideas presented in this paper. Section 3 walks through the proposed methodology, MBLE, in detail. Section 4 discusses implementation details of MBLE and experimental results. Section 5 concludes this paper.

\section{Background}
\subsection{Reinforcement Learning}
\subsection{Deep Deterministic Policy Gradient}
\subsection{Trust Region Policy Optimization}

\section{Methodology}
\subsection{Overview}

\subsection{Forward Dynamic Model Learning}
\subsection{Trajectory Sampling}
\subsection{Action Selection}

\section{Experiments}


\begin{algorithm}[h]
\scriptsize
\caption{Model-based Lookahead Exploration}\label{alg::adv_exp}
\begin{algorithmic}[1]
\State Given a pre-defined $\mu$ and $\sigma$ for noise
\State Given a pre-trained forward dynamic model $f(s, a)$, a reward function $r(s, a, s^\prime)$
\State Initialize all the initial states $\hat{s}^{1:N_{actor}}_0$ of actor with the given $s_0$
\State Initialize a off-policy model-free RL policy $\pi(s, \theta)$ %RL policy $\pi(x; \theta_{RL})$
\State Initialize the empty history actions $A^{1:N_actor}$
\State Initialize the accumulated rewards of all the actors
$R^{1:N_actor}$ as zero


\State Set constants $N_{iter}$, $N_{episode}$, $T$, and $\mathcal{T}_{P}$
\For{horizon $h=1$ to $H$}
    \For{actor $i=1$ to $N_{actor}$}
        \State $a^i_h = \pi(\hat{s}^{i}_{h-1}, \theta)$ + $\mathcal{N}(\mu, \sigma)$
        \State $\hat{s}^{i}_{h} = f(\hat{s}^{i}_{h-1}, a^i_h)$ 
        \State $r^i_h = r(\hat{s}^{i}_{h-1}, a^i_h, \hat{s}^{i}_{h})$ 
        \State $A^{i} = A^{i} \cup a^i_h$
        \State $R^{1:N_{actor}} = R^{1:N_{actor}} + r^i_h$ 
    \EndFor
\EndFor
\State $best\_idx = {argmax}_{i} R^{i}$
\State $best\_action = A^{best\_idx}_{0}$
\State return $best\_action$
\State \textbf{end}
\end{algorithmic}
\end{algorithm}


\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\end{document}
